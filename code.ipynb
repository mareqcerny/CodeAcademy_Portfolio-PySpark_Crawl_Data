{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3ba3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize a new Spark Context and read in the domain graph as an RDD.\n",
    "\n",
    "# Import required modules\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Read Domains CSV File into an RDD\n",
    "common_crawl_domain_counts = sc.textFile('./crawl/cc-main-limited-domains.csv')\n",
    "\n",
    "# Display first few domains from the RDD\n",
    "common_crawl_domain_counts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70527663",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply fmt_domain_graph_entry over common_crawl_domain_counts and save the result\n",
    "### as a new RDD named formatted_host_counts.\n",
    "\n",
    "def fmt_domain_graph_entry(entry):\n",
    "    \"\"\"\n",
    "    Formats a Common Crawl domain graph entry. Extracts the site_id, \n",
    "    top-level domain (tld), domain name, and subdomain count as seperate items.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the entry on delimiter ('\\t') into site_id, domain, tld, and num_subdomains\n",
    "    site_id, domain, tld, num_subdomains = entry.split('\\t')        \n",
    "    return int(site_id), domain, tld, int(num_subdomains)\n",
    "\n",
    "# Apply `fmt_domain_graph_entry` to the raw data RDD\n",
    "formatted_host_counts = common_crawl_domain_counts\\\n",
    ".map(lambda e: fmt_domain_graph_entry(e))\n",
    "\n",
    "# Display the first few entries of the new RDD\n",
    "formatted_host_counts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply extract_subdomain_counts over common_crawl_domain_counts and save the result\n",
    "### as a new RDD named host_counts.\n",
    "\n",
    "def extract_subdomain_counts(entry):\n",
    "    \"\"\"\n",
    "    Extract the subdomain count from a Common Crawl domain graph entry.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the entry on delimiter ('\\t') into site_id, domain, tld, and num_subdomains\n",
    "    site_id, domain, tld, num_subdomains = entry.split('\\t')\n",
    "    \n",
    "    # return ONLY the num_subdomains\n",
    "    return int(num_subdomains)\n",
    "\n",
    "\n",
    "# Apply `extract_subdomain_counts` to the raw data RDD\n",
    "host_counts = common_crawl_domain_counts\\\n",
    ".map(lambda e: extract_subdomain_counts(e))\n",
    "\n",
    "# Display the first few entries\n",
    "host_counts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e5a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using host_counts, calculate the total number of subdomains across all domains\n",
    "### in the dataset, save the result to a variable named total_host_counts.\n",
    "\n",
    "\n",
    "# Reduce the RDD to a single value, the sum of subdomains, with a lambda function\n",
    "# as the reduce function\n",
    "total_host_counts = host_counts\\\n",
    ".reduce(lambda a,b: a+b)\n",
    "\n",
    "# Display result count\n",
    "total_host_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3014dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stop the current SparkSession and sparkContext before moving on to analyze the data with SparkSQL\n",
    "\n",
    "# Stop the sparkContext and the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ed6091",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a new SparkSession and assign it to a variable named spark.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read ./crawl/cc-main-limited-domains.csv into a new Spark DataFrame named common_crawl.\n",
    "\n",
    "# Read the target file into a DataFrame\n",
    "common_crawl = spark \\\n",
    ".read \\\n",
    ".option('delimiter', '\\t') \\\n",
    ".option('inferSchema', 'True') \\\n",
    ".csv('./crawl/cc-main-limited-domains.csv')\n",
    "\n",
    "\n",
    "# Display the DataFrame to the notebook\n",
    "common_crawl.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cfb3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rename the DataFrame's columns to the following: site_id, domaintop_level_domain, num_subdomains\n",
    "\n",
    "# Rename the DataFrame's columns with `withColumnRenamed()`\n",
    "common_crawl = common_crawl \\\n",
    ".withColumnRenamed('_c0', 'site_id') \\\n",
    ".withColumnRenamed('_c1', 'domain') \\\n",
    ".withColumnRenamed('_c2', 'top_level_domain') \\\n",
    ".withColumnRenamed('_c3', 'num_subdomains')\n",
    "\n",
    "# Display the first few rows of the DataFrame and the new schema\n",
    "\n",
    "common_crawl.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8bffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the common_crawl DataFrame as parquet files in a directory called ./results/common_crawl/.\n",
    "\n",
    "# Save the `common_crawl` DataFrame to a series of parquet files\n",
    "\n",
    "common_crawl \\\n",
    ".write \\\n",
    ".parquet('./results/common_crawl', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd018756",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read ./results/common_crawl/ into a new DataFrame to confirm our DataFrame was saved properly.\n",
    "\n",
    "# Read from parquet directory\n",
    "common_crawl_domains = spark.read \\\n",
    ".parquet('./results/common_crawl/')\n",
    "\n",
    "# Display the first few rows of the DataFrame and the schema\n",
    "common_crawl_domains.show(5, truncate=False)\n",
    "common_crawl_domains.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040eec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a local temporary view from common_crawl_domains\n",
    "\n",
    "# Create a temporary view in the metadata for this `SparkSession`\n",
    "common_crawl_domains.createOrReplaceTempView('crawl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3564d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate the total number of domains for each top-level domain in the dataset.\n",
    "\n",
    "# Aggregate the DataFrame using DataFrame methods\n",
    "common_crawl_domains \\\n",
    ".groupby('top_level_domain') \\\n",
    ".count() \\\n",
    ".orderBy('count', ascending=False) \\\n",
    ".show(10, truncate=False)\n",
    "\n",
    "# Aggregate the DataFrame using SQL\n",
    "\n",
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT top_level_domain, count(num_subdomains)\n",
    "FROM crawl\n",
    "GROUP BY top_level_domain\n",
    "ORDER BY count(num_subdomains) DESC\n",
    "\"\"\"\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac734f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate the total number of subdomains for each top-level domain in the dataset.\n",
    "\n",
    "# Aggregate the DataFrame using DataFrame methods\n",
    "common_crawl_domains \\\n",
    ".groupby('top_level_domain') \\\n",
    ".sum('num_subdomains') \\\n",
    ".orderBy('sum(num_subdomains)', ascending=False) \\\n",
    ".show(10, truncate=False)\n",
    "\n",
    "# Aggregate the DataFrame using SQL\n",
    "\n",
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT top_level_domain, SUM(num_subdomains)\n",
    "FROM crawl\n",
    "GROUP BY top_level_domain\n",
    "ORDER BY SUM(num_subdomains) DESC\n",
    "\"\"\"\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3385984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### How many sub-domains does nps.gov have? Filter the dataset to that website's entry,\n",
    "### display the columns top_level_domain, domain, and num_subdomains in your result.\n",
    "\n",
    "# Filter the DataFrame using DataFrame Methods\n",
    "common_crawl_domains \\\n",
    ".select(['top_level_domain', 'domain', 'num_subdomains']) \\\n",
    ".filter(common_crawl_domains.domain == 'nps') \\\n",
    ".filter(common_crawl_domains.top_level_domain == 'gov') \\\n",
    ".show(5, truncate=False)\n",
    "\n",
    "# Filter the DataFrame using SQL\n",
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT top_level_domain, domain, num_subdomains\n",
    "FROM crawl\n",
    "WHERE domain = 'nps' and top_level_domain = 'gov'\n",
    "\"\"\"\n",
    ").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0cc369",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Close the SparkSession and underlying sparkContext\n",
    "\n",
    "# Stop the notebook's `SparkSession` and `sparkContext`\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
